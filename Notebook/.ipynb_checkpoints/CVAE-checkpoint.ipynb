{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf79383-a1e7-4b7f-8039-ef3bab313de8",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf3f33d-bc99-42cb-975e-413c4ad90467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# processes input image and flattens feature maps\n",
    "def get_conditional_encoder1():\n",
    "    inputs = tf.keras.Input(shape = (25,25,1))\n",
    "    #x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu')(inputs)\n",
    "    #x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs,outputs=[x])\n",
    "\n",
    "# gets flattened feature maps, and one hot label vector and outputs mu and rho\n",
    "def get_conditional_encoder2(latent_dim,input_size):\n",
    "    inputs = tf.keras.Input(shape = (input_size + 23,))\n",
    "    x = tf.keras.layers.Dense(units=400, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Dense(units=200, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(units=50, activation='relu')(x)    \n",
    "    mu = tf.keras.layers.Dense(units=latent_dim)(x)\n",
    "    rho = tf.keras.layers.Dense(units=latent_dim)(x)\n",
    "\n",
    "    return  tf.keras.Model(inputs=inputs,outputs=[mu,rho])\n",
    "\n",
    "# classical vae decoder\n",
    "def get_conditional_decoder(latent_dim):\n",
    "    z = tf.keras.Input(shape = (latent_dim+23,))\n",
    "    x= tf.keras.layers.Dense(units=50, activation='relu')(z)\n",
    "    x= tf.keras.layers.Dense(units=200, activation='relu')(x)\n",
    "    x= tf.keras.layers.Dense(units=400, activation='relu')(x)\n",
    "    x= tf.keras.layers.Dense(units=625, activation='softmax')(x)\n",
    "    decoded_img=tf.keras.layers.Reshape(target_shape=(25, 25, 1))(x)  \n",
    "    return tf.keras.Model(inputs=z,outputs=[decoded_img])\n",
    "\n",
    "class Conditional_VAE(tf.keras.Model):\n",
    "    def __init__(self,latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_block1 = get_conditional_encoder1()\n",
    "        # 2304 is specific to conv layers, not the best practice to hardcode it\n",
    "        self.encoder_block2 = get_conditional_encoder2(latent_dim=latent_dim,input_size=625)\n",
    "        self.decoder_block = get_conditional_decoder(latent_dim)\n",
    "\n",
    "    def call(self,img,labels):\n",
    "        # encoder q(z|x,y)\n",
    "        enc1_output = self.encoder_block1(img)\n",
    "        # concat feature maps and one hot label vector\n",
    "        img_lbl_concat = np.concatenate((enc1_output,labels),axis=1)\n",
    "        z_mu,z_rho = self.encoder_block2(img_lbl_concat)\n",
    "\n",
    "        # sampling\n",
    "        epsilon = tf.random.normal(shape=z_mu.shape,mean=0.0,stddev=1.0)\n",
    "        z = z_mu + tf.math.softplus(z_rho) * epsilon\n",
    "\n",
    "        # decoder p(x|z,y)\n",
    "        z_lbl_concat = np.concatenate((z,labels),axis=1)\n",
    "        decoded_img = self.decoder_block(z_lbl_concat)\n",
    "\n",
    "        return z_mu,z_rho,decoded_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805bc220-5967-4c64-9e63-3a3fd46828fc",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf8a99d-db4b-4d33-9695-b882657fe43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# closed form kl loss computation between variational posterior q(z|x) and unit Gaussian prior p(z) \n",
    "def kl_loss(z_mu,z_rho):\n",
    "    sigma_squared = tf.math.softplus(z_rho) ** 2\n",
    "    kl_1d = -0.5 * (1 + tf.math.log(sigma_squared) - z_mu ** 2 - sigma_squared)\n",
    "\n",
    "    # sum over sample dim, average over batch dim\n",
    "    kl_batch = tf.reduce_mean(tf.reduce_sum(kl_1d,axis=1))\n",
    "\n",
    "    return kl_batch\n",
    "\n",
    "def elbo(z_mu,z_rho,decoded_img,original_img):\n",
    "    # reconstruction loss\n",
    "    mse = tf.reduce_mean(tf.reduce_sum(tf.square(original_img - decoded_img),axis=1))\n",
    "    # kl loss\n",
    "    kl = kl_loss(z_mu,z_rho)\n",
    "\n",
    "    return mse,kl\n",
    "\n",
    "\n",
    "\n",
    "def train(latent_dim,beta,epochs,train_ds):\n",
    "\n",
    "    model = Conditional_VAE(latent_dim)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "    kl_loss_tracker = tf.keras.metrics.Mean(name='kl_loss')\n",
    "    mse_loss_tracker = tf.keras.metrics.Mean(name='mse_loss')\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        label_list = None\n",
    "        z_mu_list = None    \n",
    "\n",
    "        for _,(imgs,labels) in train_ds.enumerate():\n",
    "            \n",
    "            # training loop\n",
    "            with tf.GradientTape() as tape:\n",
    "                # forward pass\n",
    "                z_mu,z_rho,decoded_imgs = model(imgs,labels)\n",
    "\n",
    "                # compute loss\n",
    "                mse,kl = elbo(z_mu,z_rho,decoded_imgs,imgs)\n",
    "                loss = mse + beta * kl\n",
    "            \n",
    "            # compute gradients\n",
    "            gradients = tape.gradient(loss,model.variables)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "\n",
    "            # update metrics\n",
    "            kl_loss_tracker.update_state(kl)\n",
    "            mse_loss_tracker.update_state(mse)\n",
    "\n",
    "            # save encoded means and labels for latent space visualization\n",
    "            if label_list is None:\n",
    "                label_list = labels\n",
    "            else:\n",
    "                label_list = np.concatenate((label_list,labels))\n",
    "                \n",
    "            if z_mu_list is None:\n",
    "                z_mu_list = z_mu\n",
    "            else:\n",
    "                z_mu_list = np.concatenate((z_mu_list,z_mu),axis=0)\n",
    "\n",
    "    \n",
    "        # display metrics at the end of each epoch.\n",
    "        epoch_kl,epoch_mse = kl_loss_tracker.result(),mse_loss_tracker.result()\n",
    "        print(f'epoch: {epoch}, mse: {epoch_mse:.4f}, kl_div: {epoch_kl:.4f}')\n",
    "\n",
    "        # reset metric states\n",
    "        kl_loss_tracker.reset_state()\n",
    "        mse_loss_tracker.reset_state()\n",
    "\n",
    "    return model,z_mu_list,label_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4f6bd-01c4-4f89-8295-121ebccbba87",
   "metadata": {},
   "source": [
    "## Ingest data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "659c5ed4-93da-4e90-8470-9aff23f7f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def prepare_sequence_data():\n",
    "    def read_data_from_csv(file:str = 'prepared_data.csv'):\n",
    "        df = pd.read_csv(file)\n",
    "        df['Sequences'] = df['Sequences'].apply(literal_eval)\n",
    "        df['Durations'] = df['Durations'].apply(literal_eval)\n",
    "        one_hot = pd.get_dummies(df.BodyPart).astype(int)\n",
    "        df['BodyPart_encoded'] = one_hot.values.tolist()\n",
    "        return df\n",
    "    \n",
    "    df = read_data_from_csv()\n",
    "    x_train = np.array([x for x in df.Sequences.values])    \n",
    "    x_train = np.expand_dims(x_train,axis=3)\n",
    "    x_train = tf.cast(x_train,dtype=tf.float32)\n",
    "    \n",
    "    y_train = [x for x in df.BodyPart_encoded.values]\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "    train_ds = train_ds.shuffle(1000).batch(64)\n",
    "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f060680-b8f0-494b-83b0-08b1d58f37a7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21b63dc1-f0a2-485e-b842-30980ca84724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, mse: 0.4188, kl_div: 1.6967\n",
      "epoch: 1, mse: 0.4237, kl_div: 1.7033\n",
      "epoch: 2, mse: 0.4110, kl_div: 1.7011\n",
      "epoch: 3, mse: 0.4053, kl_div: 1.6946\n",
      "epoch: 4, mse: 0.4211, kl_div: 1.7013\n",
      "epoch: 5, mse: 0.4101, kl_div: 1.7002\n",
      "epoch: 6, mse: 0.4251, kl_div: 1.7027\n",
      "epoch: 7, mse: 0.4196, kl_div: 1.6964\n",
      "epoch: 8, mse: 0.4040, kl_div: 1.7016\n",
      "epoch: 9, mse: 0.4103, kl_div: 1.7010\n",
      "epoch: 10, mse: 0.4314, kl_div: 1.7041\n",
      "epoch: 11, mse: 0.4088, kl_div: 1.6943\n",
      "epoch: 12, mse: 0.4033, kl_div: 1.6944\n",
      "epoch: 13, mse: 0.4032, kl_div: 1.7013\n",
      "epoch: 14, mse: 0.4003, kl_div: 1.6942\n",
      "epoch: 15, mse: 0.4065, kl_div: 1.7016\n",
      "epoch: 16, mse: 0.4025, kl_div: 1.7056\n",
      "epoch: 17, mse: 0.3773, kl_div: 1.6936\n",
      "epoch: 18, mse: 0.3844, kl_div: 1.7044\n",
      "epoch: 19, mse: 0.3795, kl_div: 1.6976\n",
      "epoch: 20, mse: 0.3905, kl_div: 1.7063\n",
      "epoch: 21, mse: 0.3792, kl_div: 1.7007\n",
      "epoch: 22, mse: 0.4046, kl_div: 1.7037\n",
      "epoch: 23, mse: 0.3757, kl_div: 1.6994\n",
      "epoch: 24, mse: 0.3898, kl_div: 1.6999\n",
      "epoch: 25, mse: 0.4034, kl_div: 1.6975\n",
      "epoch: 26, mse: 0.3947, kl_div: 1.7006\n",
      "epoch: 27, mse: 0.3852, kl_div: 1.7011\n",
      "epoch: 28, mse: 0.3962, kl_div: 1.6982\n",
      "epoch: 29, mse: 0.3957, kl_div: 1.7025\n",
      "epoch: 30, mse: 0.3822, kl_div: 1.6980\n",
      "epoch: 31, mse: 0.3916, kl_div: 1.7020\n",
      "epoch: 32, mse: 0.3872, kl_div: 1.6989\n",
      "epoch: 33, mse: 0.3792, kl_div: 1.7025\n",
      "epoch: 34, mse: 0.3822, kl_div: 1.7058\n",
      "epoch: 35, mse: 0.3927, kl_div: 1.6990\n",
      "epoch: 36, mse: 0.3787, kl_div: 1.7077\n",
      "epoch: 37, mse: 0.3851, kl_div: 1.6980\n",
      "epoch: 38, mse: 0.3944, kl_div: 1.7048\n",
      "epoch: 39, mse: 0.3842, kl_div: 1.7027\n",
      "epoch: 40, mse: 0.3815, kl_div: 1.7025\n",
      "epoch: 41, mse: 0.3667, kl_div: 1.7021\n",
      "epoch: 42, mse: 0.3772, kl_div: 1.7013\n",
      "epoch: 43, mse: 0.3997, kl_div: 1.7048\n",
      "epoch: 44, mse: 0.3848, kl_div: 1.6969\n",
      "epoch: 45, mse: 0.3812, kl_div: 1.7012\n",
      "epoch: 46, mse: 0.3749, kl_div: 1.7043\n",
      "epoch: 47, mse: 0.3846, kl_div: 1.7012\n",
      "epoch: 48, mse: 0.3716, kl_div: 1.6991\n",
      "epoch: 49, mse: 0.3800, kl_div: 1.6956\n",
      "epoch: 50, mse: 0.3778, kl_div: 1.7024\n",
      "epoch: 51, mse: 0.3850, kl_div: 1.7038\n",
      "epoch: 52, mse: 0.3747, kl_div: 1.6962\n",
      "epoch: 53, mse: 0.3669, kl_div: 1.7056\n",
      "epoch: 54, mse: 0.3763, kl_div: 1.6973\n",
      "epoch: 55, mse: 0.3785, kl_div: 1.7036\n",
      "epoch: 56, mse: 0.3658, kl_div: 1.7018\n",
      "epoch: 57, mse: 0.3814, kl_div: 1.7024\n",
      "epoch: 58, mse: 0.3745, kl_div: 1.6971\n",
      "epoch: 59, mse: 0.3854, kl_div: 1.7070\n",
      "epoch: 60, mse: 0.3715, kl_div: 1.6951\n",
      "epoch: 61, mse: 0.3799, kl_div: 1.7039\n",
      "epoch: 62, mse: 0.3708, kl_div: 1.7041\n",
      "epoch: 63, mse: 0.3733, kl_div: 1.7040\n",
      "epoch: 64, mse: 0.3902, kl_div: 1.7035\n",
      "epoch: 65, mse: 0.3772, kl_div: 1.7015\n",
      "epoch: 66, mse: 0.3873, kl_div: 1.7039\n",
      "epoch: 67, mse: 0.3693, kl_div: 1.6983\n",
      "epoch: 68, mse: 0.3883, kl_div: 1.7071\n",
      "epoch: 69, mse: 0.3692, kl_div: 1.7022\n",
      "epoch: 70, mse: 0.3808, kl_div: 1.7003\n",
      "epoch: 71, mse: 0.3685, kl_div: 1.7023\n",
      "epoch: 72, mse: 0.3623, kl_div: 1.6951\n",
      "epoch: 73, mse: 0.3730, kl_div: 1.6960\n",
      "epoch: 74, mse: 0.3827, kl_div: 1.7006\n",
      "epoch: 75, mse: 0.3771, kl_div: 1.7050\n",
      "epoch: 76, mse: 0.3758, kl_div: 1.6970\n",
      "epoch: 77, mse: 0.3823, kl_div: 1.6997\n",
      "epoch: 78, mse: 0.3701, kl_div: 1.7021\n",
      "epoch: 79, mse: 0.3776, kl_div: 1.6984\n",
      "epoch: 80, mse: 0.3702, kl_div: 1.7012\n",
      "epoch: 81, mse: 0.3654, kl_div: 1.6972\n",
      "epoch: 82, mse: 0.3757, kl_div: 1.7031\n",
      "epoch: 83, mse: 0.3809, kl_div: 1.7009\n",
      "epoch: 84, mse: 0.3844, kl_div: 1.7076\n",
      "epoch: 85, mse: 0.3771, kl_div: 1.7052\n",
      "epoch: 86, mse: 0.3773, kl_div: 1.7025\n",
      "epoch: 87, mse: 0.3655, kl_div: 1.7013\n",
      "epoch: 88, mse: 0.3715, kl_div: 1.7033\n",
      "epoch: 89, mse: 0.3719, kl_div: 1.7044\n",
      "epoch: 90, mse: 0.3819, kl_div: 1.7087\n",
      "epoch: 91, mse: 0.3572, kl_div: 1.6915\n",
      "epoch: 92, mse: 0.3724, kl_div: 1.7074\n",
      "epoch: 93, mse: 0.3714, kl_div: 1.6988\n",
      "epoch: 94, mse: 0.3864, kl_div: 1.7015\n",
      "epoch: 95, mse: 0.3830, kl_div: 1.7033\n",
      "epoch: 96, mse: 0.3726, kl_div: 1.6922\n",
      "epoch: 97, mse: 0.3736, kl_div: 1.7015\n",
      "epoch: 98, mse: 0.3764, kl_div: 1.7046\n",
      "epoch: 99, mse: 0.3897, kl_div: 1.7005\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "beta = 1e-11\n",
    "epochs = 100\n",
    "latent_dim = 15\n",
    "\n",
    "train_ds = prepare_sequence_data()\n",
    "\n",
    "model,z_mu_list,label_list = train(latent_dim,beta,epochs,train_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d1d0661-579d-403e-a85c-5edbd631d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(a):\n",
    "    max_position = np.unravel_index(np.argmax(a), a.shape)       \n",
    "    result = np.zeros_like(a)\n",
    "    if max(a)> sum(a) * 0.2:\n",
    "        result[max_position] = 1\n",
    "    return result.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df28bfb7-3555-4a3b-8754-45d98a906792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_onehot = np.zeros((1,23))\n",
    "label_onehot[:,5] = 1.0\n",
    "\n",
    "z = tf.random.normal(shape=(1,model.encoder_block2.output[0].shape[1]),mean=0.0,stddev=1.0)\n",
    "z_lbl_concat = np.concatenate((z,label_onehot),axis=1)\n",
    "z_lbl_concat\n",
    "result = model.decoder_block(z_lbl_concat).numpy().reshape(-1, 25, 25)\n",
    "result\n",
    "[list(find_max(y)) for x in result for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47eadb9-9594-4fdb-9eff-15c3cadf82d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
